\documentclass{beamer}
\input{../utils/preamble}
\createdgmtitle{13}

%--------------------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
\titlepage
	\resetonslide
\end{frame}
%=======
\begin{frame}{Recap of Previous Lecture}
    \myfootnotewithlink{https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching}{image credit: A Visual Dive into Conditional Flow Matching}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{figs/cfm_uncond_to_cond}
	\end{figure}
	\vspace{-0.3cm}
	\begin{block}{Constraints}
		\vspace{-0.3cm}
		\[
			p(\bx) = \cN(0, \bI) = \bbE_{p(\bz)} p_0(\bx | \bz); \quad \pd(\bx) = \bbE_{p(\bz)} p_1(\bx | \bz).
		\]
		\vspace{-0.5cm}
	\end{block}
	\begin{itemize}
		\item How should we choose the conditioning latent variable $\bz$?
		\item How can we define $p_t(\bx | \bz)$ so that it meets the constraints?
	\end{itemize}
	\begin{block}{Gaussian Conditional Probability Path}
		\vspace{-0.3cm}
		\[
			p_t(\bx | \bz) = \cN\left(\bmu_t(\bz), \bsigma_t^2(\bz)\right)
		\]
		\[
			\bx_t = \bmu_t(\bz) + \bsigma_t(\bz) \odot \bx_0, \quad {\color{violet} \bx_0 \sim p_0(\bx) = \cN(0, \bI)}
		\]
	\end{block}
\end{frame}
%=======
\begin{frame}{Recap of Previous Lecture}
    \myfootnotewithlink{https://arxiv.org/abs/2210.02747}{Lipman Y., et al. Flow Matching for Generative Modeling, 2022}
	\begin{block}{Gaussian Conditional Probability Path}
		\vspace{-0.3cm}
		\[
			p_t(\bx | \bz) = \cN\left(\bmu_t(\bz), \bsigma_t^2(\bz)\right); \quad \bx_t = \bmu_t(\bz) + \bsigma_t(\bz) \odot \bx_0
		\]
		\vspace{-0.3cm}
		\[
			\bff(\bx, \bz, t) =  \bmu_t'(\bz) + \frac{\bsigma_t'(\bz)}{\bsigma_t(\bz)} \odot (\bx - \bmu_t(\bz))
		\]
		\vspace{-0.3cm}
	\end{block}
	\begin{block}{Conditioning Latent Variable}
		Let’s choose $\bz = \bx_1$. Then $p(\bz) = p_1(\bx_1)$.
		\[
			p_t(\bx) = \int p_t(\bx | \bx_1) p_1(\bx_1) d \bx_1
		\]
		\vspace{-0.5cm}
	\end{block}
	We must ensure the boundary constraints:
	\[
		\begin{cases}
			p(\bx) = \bbE_{p(\bz)} p_0(\bx | \bz); {\color{gray}(= \cN(0, \bI))} \\
			\pd(\bx) = \bbE_{p(\bz)} p_1(\bx | \bz).
		\end{cases}
		\quad \Rightarrow \quad 
		\begin{cases}
			p_0(\bx | \bx_1) = \cN(0, \bI); \\
			p_1(\bx | \bx_1) = \delta(\bx - \bx_1).
		\end{cases}
	\]
	\vspace{-0.3cm}
\end{frame}
%=======
\begin{frame}{Recap of Previous Lecture}
    \myfootnotewithlink{https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching}{image credit: A Visual Dive into Conditional Flow Matching}
	\[
		p_0(\bx | \bx_1) = \cN(0, \bI); \quad p_1(\bx | \bx_1) = \delta(\bx - \bx_1).
	\]
	
	\begin{block}{Gaussian Conditional Probability Path}
		\vspace{-0.5cm}
		\[
			p_t(\bx | \bx_1) = \cN\left(\bmu_t(\bx_1), \bsigma_t^2(\bx_1)\right); \quad \bx_t = \bmu_t(\bx_1) +  \bsigma_t(\bx_1) \odot \bx_0.
		\]
		\vspace{-0.6cm}
	\end{block}
	Let’s consider straight conditional paths:	
	\[
		\begin{cases}
			\bmu_t(\bx_1) = t \bx_1; \\
			\bsigma_t(\bx_1) = 1 - t.
		\end{cases}
		\quad \Rightarrow \quad 
		\begin{cases}
			p_t(\bx | \bx_1) = \cN\left(t \bx_1, (1-t)^2 \bI\right); \\
		 	\bx_t = t \bx_1 + (1 - t) \bx_0. 
	 \end{cases}
	\]
	\vspace{-0.3cm}
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{figs/conical_paths}
	\end{figure}
\end{frame}
%=======
\begin{frame}{Outline}
	\tableofcontents
\end{frame}
%=======
\section{Discrete Diffusion Models}
%=======
\begin{frame}{Discrete or Continuous Diffusion Models?}
	\textbf{Reminder:} Diffusion models define a forward corruption process and a reverse denoising process.
	Previously, we studied diffusion models with continuous states $\bx(t) \in \mathbb{R}^m$.
	\begin{block}{Continuous state space}
		\begin{itemize}
			\item \textbf{Discrete time} $t \in \{0,1,\ldots,T\}$ $\;\Rightarrow\;$ \textbf{DDPM / NCSN}.
			\item \textbf{Continuous time} $t \in [0,1]$ $\;\Rightarrow\;$ \textbf{Score-based SDE models}.
		\end{itemize}
	\end{block}
	\eqpause
	Now we turn to diffusion over discrete-value states $\bx(t) \in \{1, \dots, K\}^m$.
	\begin{block}{Discrete state space}
		\begin{itemize}
			\item \textbf{Discrete time} $t \in \{0,1,\ldots,T\}$.
			\item \textbf{Continuous time} $t \in [0,1]$.
		\end{itemize}
	\end{block}
	Let's discuss why we need discrete diffusion models.
\end{frame}
%=======
\begin{frame}{Why Discrete Diffusion Models?}
	\myfootnotewithlink{https://aaronlou.com/blog/2024/discrete-diffusion/}{https://aaronlou.com/blog/2024/discrete-diffusion/}
	While autoregressive (AR) models dominate discrete-data domains (e.g., text or sequences), they have fundamental limitations.
	\eqpause
	\begin{block}{Key advantages of discrete diffusion}
		\begin{itemize}
			\item \textbf{Parallel generation:} diffusion enables sampling all tokens simultaneously, unlike AR’s strictly left-to-right process. 
			\eqpause
			\item \textbf{Flexible infilling:}. diffusion can mask arbitrary parts of a sequence and reconstruct them, rather than generating only from prefix to suffix.
			\eqpause 
			\item \textbf{Robustness:} diffusion avoids the "exposure bias" caused by teacher forcing in AR training.
			\eqpause
			\item \textbf{Unified framework:} diffusion generalizes naturally to discrete domains that do not suit continuous Gaussian noise.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{2025 – Big Bang of Discrete Diffusion Models}
	\myfootnotewithlink{https://arxiv.org/abs/2506.17298}{Khanna S. et al. Mercury: Ultra-fast language models based on diffusion, 2025.}
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/mercury}
	\end{figure}
\end{frame}
\subsection{Forward Discrete Process}
%=======
\begin{frame}{Forward Discrete Process}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al. Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{block}{Continuous Diffusion Markov Chain}
		In continuous diffusion, the forward Markov chain is defined by progressively corrupting data with Gaussian noise:
		\[
			q(\bx_t|\bx_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\bx_{t-1}, \beta_t \bI).
		\]
	\end{block}
	\eqpause
	\vspace{-0.3cm}
	\begin{block}{Discrete Diffusion Markov Chain}
		For discrete data, we instead define a Markov chain over categorical states:
		\[
			q(\bx_t|\bx_{t-1}) = \Cat(\bQ_t\bx_{t-1}),
		\]
	\end{block}
	\vspace{-0.3cm}
	\eqpause
	\begin{itemize}
		\item Each $\bx_t \in \{0,1\}^K$ is a \textbf{one-hot vector} encoding the categorical state (it is just one token).
		\item What is the transition matrix $\bQ_t$?
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Forward Process over Time}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al. Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{block}{Transition Matrix}
		$\bQ_t \in [0,1]^{K \times K}$ is a \textbf{transition matrix} where each column gives transition probabilities from one state to all others, and columns sum to 1:
		\vspace{-0.5cm}
		\[
			[\bQ_t]_{ij} = q(x_t = i | x_{t-1} = j),
			\qquad
			\sum_{i=1}^K [\bQ_t]_{ij} = 1.
		\]
		\vspace{-0.7cm}
	\end{block}
	\eqpause
	\begin{itemize}
		\item The forward diffusion gradually destroys information through repeated random transitions.
		\eqpause
		\item Applying the transition $t$ times yields the marginal distribution:
			\[
				q(\bx_t|\bx_0) = \Cat(\bQ_{1:t}\bx_0),
				\qquad
				\bQ_{1:t} = \bQ_t \bQ_{t-1}\cdots\bQ_1.
			\]
		\eqpause
		\vspace{-0.7cm}
		\item As $t \to T$, the process drives the data toward a stationary distribution.
		\eqpause
		\item We design the transition matrices $\bQ_t$ to achieve this behavior.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al. Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{itemize}
		\item The choice of $\bQ_t$ determines how information is erased and what the stationary distribution becomes.
		\eqpause
		\item $\bQ_t$ and $\bQ_{1:t}$ should be easy to compute for each $t$.
	\end{itemize}
	\eqpause
	\begin{block}{Common choices}
		\begin{itemize}
			\item \textbf{Uniform diffusion}
			\[
				\bQ_t = (1 - \beta_t) \bI + \beta_t \bU,
				\qquad
				\mathbf{U}_{ij} = \tfrac{1}{K}.
			\]
			Each token is replaced by a uniformly random symbol with probability $\beta_t$.
			The stationary distribution is uniform noise.
			\eqpause
			\item \textbf{Absorbing diffusion}
			\[
				\bQ_t = (1-\beta_t)\bI + \beta_t\, \mathbf{e}_m \mathbf{1}^\top.
			\]
			Tokens are gradually replaced by a special mask $m$;
			the stationary distribution is fully masked.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\[
		q(\bx_t|\bx_0) = \Cat(\bQ_{1:t}\bx_0),
		\qquad
		\bQ_{1:t} = \bQ_t \bQ_{t-1} \cdots \bQ_1.
	\]
	\eqpause
	\begin{block}{Uniform Diffusion}
		\[
			\bQ_t = (1-\beta_t)\bI + \beta_t \mathbf{U},
			\qquad
			\mathbf{U}_{ij} = \tfrac{1}{K}.
		\]
		\[
			\bQ_{1:t} = \bar\alpha_t\bI + (1 - \bar\alpha_t) \mathbf{U},
			\quad
			\bar\alpha_t = \prod_{s=1}^t (1-\beta_s).
		\]
		\eqpause
		\begin{itemize}
			\item Each token retains its original value with prob.~$\bar\alpha_t$.
			\item It becomes uniformly random with prob.~$(1 - \bar\alpha_t)$.
			\item As $t \to T$, the process converges to the stationary uniform distribution.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

	\begin{block}{Absorbing Diffusion}
		\[
			\bQ_t = (1-\beta_t)\bI + \beta_t\, \mathbf{e}_m \mathbf{1}^\top,
		\]
		\[
			\bQ_{1:t} = \bar\alpha_t\,\bI + (1-\bar\alpha_t)\,\mathbf{e}_m \mathbf{1}^\top,
			\qquad
			\bar\alpha_t = \prod_{s=1}^t (1-\beta_s).
		\]
		\eqpause
		\begin{itemize}
			\item Each token retains its original value with prob.~$\bar\alpha_t$.
			\item It becomes $\mathbf{e}_m$ with prob.~$(1 - \bar\alpha_t)$.
			\item As $t \to T$, all tokens converge to the mask state: $q(\bx_T) \approx \Cat(\mathbf{e}_m)$.
			\item This makes the process analogous to \textbf{masked language modeling}.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{Uniform vs. Absorbing Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{table}[h!]
		\centering
		\small
		\begin{tabular}{lcc}
			\toprule
			\textbf{Aspect} & \textbf{Uniform Diffusion} & \textbf{Absorbing Diffusion} \\
			\midrule
			$\bQ_t$
				& $(1-\beta_t)\bI + \beta_t \mathbf{U}$ 
				& $(1-\beta_t)\bI + \beta_t \mathbf{e}_m\mathbf{1}^\top$ \\[4pt]
			$\bQ_{1:t}$
				& $\bar\alpha_t \bI + (1-\bar\alpha_t)\mathbf{U}$
				& $\bar\alpha_t \bI + (1-\bar\alpha_t)\mathbf{e}_m\mathbf{1}^\top$ \\[4pt]
			$\bQ_{1:\infty}$
				& $\mathbf{U}$
				& $\Cat(\mathbf{e}_m)$ \\[4pt]
			Interpretation
				& Random replacement
				& Gradual masking of tokens \\[4pt]
			Application
				& Image / symbol diffusion
				& Text diffusion $\approx$ Masked LM \\[4pt]
			\bottomrule
		\end{tabular}
	\end{table}
	\eqpause
	\begin{block}{Observation}
		Both schemes gradually destroy information, but differ in their stationary limit.
		Absorbing diffusion bridges diffusion and masked-language-model objectives.
	\end{block}
\end{frame}
%=======
\subsection{Reverse Discrete Diffusion}
%=======
\begin{frame}{Posterior of the Forward Process}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{block}{ELBO}
		\vspace{-0.7cm}
		\begin{multline*}
            \cL_{\bphi, \btheta}(\bx) =  {\color{olive}\bbE_{q(\bx_1 | \bx_0)} \log \pt(\bx_0 | \bx_1)} - {\color{violet}\KL\bigl(q(\bx_T | \bx_0) \| p(\bx_T)\bigr)} - \\
            - {\color{teal}\sum_{t=2}^\top \underbrace{ \bbE_{q(\bx_t | \bx_0)} \KL \bigl(q(\bx_{t-1} | \bx_t, \bx_0) \| \pt(\bx_{t-1} | \bx_t)\bigr)}_{\cL_t}}
        \end{multline*}
		\vspace{-0.7cm}
	\end{block}
	\eqpause
	\begin{itemize}
		\item Conditioned reverse distribution $q(\bx_{t-1} | \bx_t, \bx_0)$ played crucial role in the continuous-state diffusion model.
		\item It shows the probability of a previous state given the noisy state $\bx_t$ and the original clean data $\bx_0$.
	\end{itemize}
	\eqpause
	\begin{block}{Discrete conditioned reverse distribution}
		\vspace{-0.7cm}
		\begin{align*}
			q(\bx_{t-1} | \bx_t, \bx_0)
			&= \frac{q(\bx_t | \bx_{t-1}, \bx_0)\, q(\bx_{t-1} | \bx_0)}
			        {q(\bx_t | \bx_0)} \\
			&= \frac{\Cat(\bQ_t) \cdot \Cat(\bQ_{1:t-1})}{\Cat(\bQ_{1:t})}.
		\end{align*}
	\end{block}
\end{frame}
%=======
\begin{frame}{Posterior of the Forward Process}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{block}{Discrete conditioned reverse distribution}
		\[
			q(\bx_{t-1} | \bx_t, \bx_0)
			= \Cat\left(\frac{\bQ_t \bx_t \odot \bQ_{1:t-1} \bx_0}{\bx_t^{\top}\bQ_{1:t}\bx_0}\right).
		\]
		\vspace{-0.3cm}
	\end{block}
	\eqpause
	Recall the ELBO term
	\[
		\cL_t
		= \bbE_{q(\bx_t | \bx_0)}
		\KL\bigl(q(\bx_{t-1} | \bx_t, \bx_0) \,\|\, \pt(\bx_{t-1} | \bx_t)\bigr),
	\]
	\eqpause
	\vspace{-0.5cm}
	\begin{itemize}
		\item Both $q(\bx_{t-1} | \bx_t, \bx_0)$ and $q(\bx_t | \bx_0)$ are known analytically from the forward process.
		\item The reverse process $\pt(\bx_{t-1} | \bx_t)$ is a learned categorical distribution:
		\[
			\pt(\bx_{t-1} | \bx_t) = \Cat\bigl( \bpi_{\btheta}(\bx_t, t) \bigr),
		\]
		where $\bpi_{\btheta}$ is a neural network.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Discrete-time ELBO for Discrete Diffusion}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

	\begin{block}{ELBO term}
		\vspace{-0.3cm}
		\[
			\cL_t
			= \bbE_{q(\bx_t | \bx_0)}
			\KL\bigl(q(\bx_{t-1} | \bx_t, \bx_0) \,\|\, \pt(\bx_{t-1} | \bx_t)\bigr).
		\]
		\vspace{-0.3cm}
	\end{block}
	\eqpause
	\begin{block}{Categorical KL}
		\vspace{-0.3cm}
		\[
			\KL\bigl(\Cat(\bq) \,\|\, \Cat(\bp)\bigr)
			= \sum_{k=1}^K q_k \log \frac{q_k}{p_k}
			= \Ent(\bq,\bp) - \Ent(\bq),
		\]
		\eqpause
		\begin{itemize}
			\item $\Ent\bigl(q(\bx_{t-1} | \bx_t, \bx_0)\bigr)$ is a constant w.r.t.~$\btheta$.
			\item $\Ent(\bq,\bp) = -\sum_k q_k \log p_k$ is a \textbf{cross-entropy loss}.
		\end{itemize}
	\end{block}
	\eqpause
	Therefore, minimizing $\cL_t$ w.r.t.~$\btheta$ is equivalent to minimizing
	\[
		\bbE_{q(\bx_t | \bx_0)}
		\Ent\Bigl(q(\bx_{t-1} | \bx_t, \bx_0),\, \pt(\bx_{t-1} | \bx_t)\Bigr).
	\]
\end{frame}
%=======
\subsection{Absorbing Diffusion}
%=======
\begin{frame}{Absorbing Diffusion: Forward Process}
    \myfootnotewithlink{https://arxiv.org/abs/2406.07524}{Sahoo S. et al. Simple and Effective Masked Diffusion Language Models, 2024}
	Let's restrict to the case of absorbing transition matrix.
	At each step $t$:
	\begin{itemize}
		\item with probability $(1-\beta_t)$ a token is kept;
		\item with probability $\beta_t$ it is replaced by the mask token $m$.
	\end{itemize}
    \vspace{-0.5cm}
	\begin{align*}
		\bQ_t &= (1-\beta_t)\bI + \beta_t\, \mathbf{e}_m \mathbf{1}^\top,
		\qquad
		\bar\alpha_t = \prod_{s=1}^t (1-\beta_s). \\
		\bQ_{1:t} &= \bar\alpha_t\,\bI + (1-\bar\alpha_t)\,\mathbf{e}_m \mathbf{1}^\top.
	\end{align*}
	Each position is either still clean or already masked:
    \vspace{-0.3cm}
	\[
		q(\bx_t | \bx_0) = 
		\begin{cases}
		\bar\alpha_t, &\bx_t = \bx_0\\
		1 - \bar\alpha_t, &\bx_t = \mathbf{e}_m \\
		0, &\text{otherwise}.
		\end{cases}
	\]
\end{frame}
%=======
\begin{frame}
	NOT READY
\end{frame}
%=======
\begin{frame}{Absorbing / Masked Diffusion: Sequence View}
    \myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

    Consider a sequence $\bx_0 = (x_0^1,\dots,x_0^L)$.

    \begin{block}{Independent masking across positions}
        Because the forward chain factorizes over positions,
        \[
            q(\bx_t \mid \bx_0) = \prod_{\ell=1}^L q(x_t^\ell \mid x_0^\ell),
        \]
        and for each position $\ell$:
        \[
            q(x_t^\ell = x_0^\ell \mid \bx_0) = \bar\alpha_t,
            \qquad
            q(x_t^\ell = m \mid \bx_0) = 1 - \bar\alpha_t.
        \]
    \end{block}

    \begin{itemize}
        \item At small $t$, most tokens remain clean; a few are masked.
        \item As $t \to T$, almost all tokens become $m$ and $q(\bx_T)$ is concentrated on the fully masked sequence.
        \item This gives a \textbf{multi-step masking schedule}, instead of BERT’s single-step masking.
    \end{itemize}
\end{frame}
%=======
\begin{frame}{Posterior in Absorbing Diffusion: Unmask vs Stay Masked}
    \myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

    Recall the general discrete posterior
    \[
        q(x_{t-1} \mid x_t, x_0)
        = \frac{q(x_t \mid x_{t-1})\, q(x_{t-1} \mid x_0)}
               {q(x_t \mid x_0)}.
    \]
    For the absorbing process we can obtain a closed-form expression.
    \begin{block}{Case 1: $x_t = x_0$ (token not yet masked)}
        Because the mask is absorbing, we cannot go from mask back to a clean token:
        \[
            q(x_{t-1} = x_0 \mid x_t = x_0, x_0) = 1.
        \]
        If we observe $x_t = x_0$, we know the token has \textbf{never been masked} up to time $t$.
    \end{block}
\end{frame}
%=======
\begin{frame}{Posterior in Absorbing Diffusion: Unmask vs Stay Masked}
    \begin{block}{Case 2: $x_t = m$ (token is masked)}
        Now $x_{t-1}$ could be:
        \begin{itemize}
            \item already masked at $t-1$ and stayed masked, or
            \item still clean ($x_0$) at $t-1$ and masked only at step $t$.
        \end{itemize}
        Using the forward marginals,
        \[
            q(x_{t-1} = x_0 \mid x_t = m, x_0)
            = \frac{\bar\alpha_{t-1}\,\beta_t}{1-\bar\alpha_t},
        \]
        \[
            q(x_{t-1} = m \mid x_t = m, x_0)
            = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t},
        \]
        and all other states have probability $0$.
    \end{block}
\end{frame}
%=======
\begin{frame}{Posterior in Absorbing Diffusion: Interpretation}
    \begin{block}{Unmask vs stay masked}
        When $x_t = m$,
        \[
            q(x_{t-1} \mid x_t = m, x_0)
            = \underbrace{\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}}_{\text{already masked}}
                \delta_{x_{t-1}=m}
            + \underbrace{\frac{\bar\alpha_{t-1}\beta_t}{1-\bar\alpha_t}}_{\text{just masked}}
                \delta_{x_{t-1}=x_0}.
        \]
    \end{block}

    \begin{itemize}
        \item The posterior is a simple binary choice:
        \begin{itemize}
            \item \textbf{stay masked}: keep $x_{t-1} = m$,
            \item \textbf{unmask}: revert to the original symbol $x_0$.
        \end{itemize}
        \item The reverse model $\pt(x_{t-1} \mid x_t)$ learns, at masked positions,
        how likely it is to \emph{unmask} vs \emph{stay masked}.
        \item This is exactly the semantic of \textbf{iterative infilling}:
        tokens start from mask and are gradually turned into meaningful symbols.
    \end{itemize}
\end{frame}
%=======
\begin{frame}{ELBO Term for Absorbing Diffusion}
    \myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

    Recall the per-timestep ELBO term
    \[
        \cL_t
        = \bbE_{q(\bx_t \mid \bx_0)}
        \KL\bigl(q(\bx_{t-1} \mid \bx_t, \bx_0)
        \,\|\, \pt(\bx_{t-1} \mid \bx_t)\bigr).
    \]

    \begin{block}{Categorical KL $\Rightarrow$ cross-entropy}
        As before,
        \[
            \KL\bigl(\Cat(\bq)\,\|\,\Cat(\bp)\bigr)
            = \Ent(\bq,\bp) - \Ent(\bq),
        \]
        and the entropy term $\Ent(\bq)$ does not depend on $\btheta$.

        Therefore minimizing $\cL_t$ is equivalent (w.r.t.~$\btheta$) to
        \[
            \bbE_{q(\bx_t \mid \bx_0)}
            \Ent\Bigl(q(\bx_{t-1} \mid \bx_t, \bx_0),
                     \pt(\bx_{t-1} \mid \bx_t)\Bigr).
        \]
    \end{block}

    \begin{itemize}
        \item For absorbing diffusion, $q(x_{t-1}^\ell \mid x_t^\ell, x_0^\ell)$
              is supported only on $\{x_0^\ell, m\}$.
        \item This makes the target distribution extremely simple, and opens the door
              to a much simpler training loss.
    \end{itemize}
\end{frame}
%=======
\begin{frame}{Simplified Training: Predict Clean Token at Masked Positions}
    \myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}
    \myfootnotewithlink{https://arxiv.org/pdf/2406.07524.pdf}{See also recent refinements in arXiv:2406.07524.}

    \begin{block}{Key observation}
        For absorbing diffusion:
        \begin{itemize}
            \item If $x_t^\ell \neq m$, then $x_t^\ell = x_0^\ell$ and the posterior
                  $q(x_{t-1}^\ell \mid x_t^\ell, x_0^\ell)$ is a delta at $x_0^\ell$.
            \item If $x_t^\ell = m$, the posterior is a binary distribution over $\{m, x_0^\ell\}$.
        \end{itemize}
        The informative supervision is concentrated at \textbf{masked positions}.
    \end{block}
\end{frame}
%=======
\begin{frame}{Simplified Training: Predict Clean Token at Masked Positions}
    \begin{block}{Practical training objective}
        In practice we parameterize the model to predict $x_0$ from $(\bx_t, t)$:
        \[
            \pt(x_0^\ell \mid \bx_t, t) = \Cat\bigl(\bpi_\theta(\bx_t, t)^\ell\bigr),
        \]
        and minimize a time-conditioned cross-entropy:
        \[
            \cL_{\text{mask}}(\theta)
            = \bbE_{t,\,\bx_0,\,\bx_t \sim q(\bx_t \mid \bx_0)}
              \sum_{\ell=1}^L w_t \,
              \mathbb{I}\{x_t^\ell = m\}
              \bigl[-\log \pt(x_0^\ell \mid \bx_t, t)\bigr].
        \]
        \begin{itemize}
            \item $w_t$ – optional weighting over timesteps (e.g., uniform over $t$).
            \item We apply cross-entropy only at positions where the input token is masked.
        \end{itemize}
    \end{block}
\end{frame}
%=======
\begin{frame}{Absorbing Diffusion as Multi-step Masked LM}
    \begin{itemize}
        \item Forward process: gradually replace tokens by a mask $m$
              according to a diffusion schedule $\{\beta_t\}$.
        \item Reverse process: starting from an all-mask sequence,
              iteratively \textbf{unmask} positions by predicting clean tokens $x_0$
              from $(\bx_t, t)$.
        \item Training: time-conditioned masked language modeling objective
              on masked positions:
              \[
                  (\bx_0, t) \;\mapsto\; \bx_t \sim q(\bx_t \mid \bx_0),
                  \quad
                  \text{predict } x_0^\ell \text{ wherever } x_t^\ell = m.
              \]
        \item This perspective makes absorbing diffusion feel very close to BERT-style
              masked LMs, but with:
              \begin{itemize}
                  \item a \textbf{multi-step} corruption schedule,
                  \item explicit modeling of the full reverse Markov chain.
              \end{itemize}
    \end{itemize}
\end{frame}
%=======

\begin{frame}{Summary}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}
\end{document}