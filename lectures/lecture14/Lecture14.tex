\documentclass{beamer}
\input{../utils/preamble}
\createdgmtitle{14}

%--------------------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
\titlepage
	\resetonslide
\end{frame}
%=======
\begin{frame}{Recap of Previous Lecture}
\end{frame}
%=======
\begin{frame}{Outline}
	\tableofcontents
\end{frame}
%=======
\section{Latent Space Models}
%=======
\subsection{Score-Based Models}
%=======
\begin{frame}{Latent Space Models}
    \myfootnote{\href{https://arxiv.org/abs/2307.08698}{Dao Q. et al. Flow Matching in Latent Space, 2023} \\ \href{https://neurips2023-ldm-tutorial.github.io/}{NeurIPS 2023 Tutorial: Latent Diffusion Models: Is the Generative AI Revolution Happening in Latent Space?}}
		\vspace{-0.3cm}
	\begin{block}{Score-Based Models (Diffusion)}
		\vspace{-0.3cm}
		\begin{figure}
			\includegraphics[width=\linewidth]{figs/latent_diffusion}
		\end{figure}
		\vspace{-0.3cm}
	\end{block}
    \eqpause
	\begin{block}{Flow Matching}
		\vspace{-0.3cm}
		\begin{figure}
			\includegraphics[width=\linewidth]{figs/latent_flow_matching}
		\end{figure}
	\end{block}
\end{frame}
%=======
\subsection{Autoregressive Models}
%=======
\begin{frame}{Vector Quantized VAE (VQ-VAE)}
    \myfootnote{\href{https://arxiv.org/abs/2004.02088}{Zhao Y. et al. Feature Quantization Improves GAN Training, 2020} \\ \href{https://arxiv.org/abs/1711.00937}{Oord A., Vinyals O., Kavukcuoglu K. Neural Discrete Representation Learning, 2017}}

	Define a dictionary space $\{\be_k\}_{k=1}^K$, where $\be_k \in \bbR^C$ and $K$ is the dictionaryâ€™s size.
	\vspace{-0.5cm}
	\begin{minipage}[t]{0.45\columnwidth}
		\[
			\bz_q = \bq (\bz) = \be_{k^*}
		\]
        Here $k^* = \argmin_k \| \bz - \be_k \|$.
	\end{minipage}%
	\begin{minipage}[t]{0.55\columnwidth}
		\vspace{-0.5cm}
		\begin{figure}
			\includegraphics[width=\linewidth]{figs/fqgan_cnn}
		\end{figure}
	\end{minipage}	
	\vspace{0.5cm}	
    \eqpause
	\[
		\cL_{\bphi, \btheta}(\bx)  =  \log \pt(\bx| \bz_q) - \log K
	\]
	\vspace{-0.3cm}
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{figs/vqvae}
	\end{figure}
\end{frame}
%=======
\begin{frame}{Vector Quantized GAN}
    \myfootnotewithlink{https://arxiv.org/abs/2012.09841}{Esser P. et al. Taming Transformers for High-Resolution Image Synthesis, 2020}
	\begin{itemize}
		\item We use a VQ-VAE model and its objective.
		\item We add an adversarial loss between generated and real images to further improve the visual quality of reconstructions.
	\end{itemize}
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/vqgan}
	\end{figure}
\end{frame}
%=======
\begin{frame}{LlamaGen: Pure Autoregression}
    \myfootnotewithlink{https://arxiv.org/abs/2406.06525}{Sun P. et al. Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation, 2024}
	\begin{itemize}
		\item Use a VQ-GAN encoder for mapping images into the discrete latent space (codebook vectors).
		\item Train a pure autoregressive model (Llama-based) in the latent space.
		\item Use the VQ-GAN decoder to map discrete tokens back to image space.
	\end{itemize}
	\vspace{-0.3cm}
	\begin{figure}
		\includegraphics[width=0.85\linewidth]{figs/llamagen_samples}
	\end{figure}
\end{frame}
%=======
\begin{frame}{Visual Autoregressive Modeling (VAR)}
    \myfootnotewithlink{https://arxiv.org/abs/2404.02905}{Tean K. et al. Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, 2024}
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{figs/var_idea}
	\end{figure}
    \eqpause
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{figs/var_training}
	\end{figure}
\end{frame}
%=======
\section{The Worst Course Overview}
%=======
\begin{frame}{The Worst Course Overview :)}
    \myfootnotewithlink{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}{Weng L. What are Diffusion Models?, blog post, 2021}
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/generative-overview}
	\end{figure}
\end{frame}
%=======
\begin{frame}{The Worst Course Overview :)}
    \myfootnote{\href{https://arxiv.org/abs/2112.07804}{Xiao Z., Kreis K., Vahdat A. Tackling the generative learning trilemma with denoising diffusion GANs, 2021} \\ \href{https://udlbook.github.io/udlbook/}{Simon J.D. Prince. Understanding Deep Learning, 2023}}
	\vspace{-0.3cm}
	\begin{figure}
		\includegraphics[width=0.45\linewidth]{figs/trilemma}
	\end{figure}
    \eqpause
	\vspace{-0.5cm}
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{figs/gen_comp_table}
	\end{figure}
\end{frame}
%=======
\begin{frame}{Summary}
	\begin{itemize}
		\item Most state-of-the-art generative models are latent variable models with either continuous or discrete latent spaces.
	\end{itemize}
\end{frame}
%=======
\end{document}