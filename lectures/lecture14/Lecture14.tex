\documentclass{beamer}
\input{../utils/preamble}
\createdgmtitle{14}

%--------------------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
\titlepage
	\resetonslide
\end{frame}
%=======
\begin{frame}{Recap of Previous Lecture}
\end{frame}
%=======
\begin{frame}{Outline}
	\tableofcontents
\end{frame}
%=======
\section{Discrete Diffusion Models}
%=======
\begin{frame}{Discrete or Continuous Diffusion Models?}
	\textbf{Reminder:} Diffusion models define a forward corruption process and a reverse denoising process.
	Previously, we studied diffusion models with continuous states $\bx(t) \in \mathbb{R}^m$.
	\begin{block}{Continuous state space}
		\begin{itemize}
			\item \textbf{Discrete time} $t \in \{0,1,\ldots,T\}$ $\;\Rightarrow\;$ \textbf{DDPM / NCSN}.
			\item \textbf{Continuous time} $t \in [0,1]$ $\;\Rightarrow\;$ \textbf{Score-based SDE models}.
		\end{itemize}
	\end{block}
	\eqpause
	Now we turn to diffusion over discrete-value states $\bx(t) \in \{1, \dots, K\}^m$.
	\begin{block}{Discrete state space}
		\begin{itemize}
			\item \textbf{Discrete time} $t \in \{0,1,\ldots,T\}$.
			\item \textbf{Continuous time} $t \in [0,1]$.
		\end{itemize}
	\end{block}
	Let's discuss why we need discrete diffusion models.
\end{frame}
%=======
\begin{frame}{Why Discrete Diffusion Models?}
	\myfootnotewithlink{https://aaronlou.com/blog/2024/discrete-diffusion/}{https://aaronlou.com/blog/2024/discrete-diffusion/}
	While autoregressive (AR) models dominate discrete-data domains (e.g., text or sequences), they have fundamental limitations.
	\eqpause
	\begin{block}{Key advantages of discrete diffusion}
		\begin{itemize}
			\item \textbf{Parallel generation:} diffusion enables sampling all tokens simultaneously, unlike ARâ€™s strictly left-to-right process. 
			\eqpause
			\item \textbf{Flexible infilling:}. diffusion can mask arbitrary parts of a sequence and reconstruct them, rather than generating only from prefix to suffix.
			\eqpause 
			\item \textbf{Robustness:} diffusion avoids the "exposure bias" caused by teacher forcing in AR training.
			\eqpause
			\item \textbf{Unified framework:} diffusion generalizes naturally to discrete domains that do not suit continuous Gaussian noise.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\subsection{Forward Discrete Process}
%=======
\begin{frame}{Forward Discrete Process}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al. Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{block}{Continuous Diffusion Markov Chain}
		In continuous diffusion, the forward Markov chain is defined by progressively corrupting data with Gaussian noise:
		\[
			q(\bx_t|\bx_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\bx_{t-1}, \beta_t \bI).
		\]
	\end{block}
	\eqpause
	\vspace{-0.3cm}
	\begin{block}{Discrete Diffusion Markov Chain}
		For discrete data, we instead define a Markov chain over categorical states:
		\[
			q(\bx_t|\bx_{t-1}) = \Cat(\bQ_t\bx_{t-1}),
		\]
		where $\bQ_t \in [0,1]^{K \times K}$ is a \textbf{transition matrix} where each column gives transition probabilities from one state to all others, and columns sum to 1:
		\vspace{-0.3cm}
		\[
			[\bQ_t]_{ij} = q(x_t = i | x_{t-1} = j),
			\qquad
			\sum_{i=1}^K [\bQ_t]_{ij} = 1.
		\]
	\end{block}
\end{frame}
%=======
\begin{frame}{Forward Process over Time}
	\begin{itemize}
		\item The forward diffusion gradually destroys information through repeated random transitions.
		\eqpause
		\item Applying the transition $t$ times yields the marginal distribution:
			\[
				q(\bx_t|\bx_0) = \Cat(\bQ_{1:t}\bx_0),
				\qquad
				\bQ_{1:t} = \bQ_t \bQ_{t-1}\cdots\bQ_1.
			\]
		\eqpause
		\vspace{-0.7cm}
		\item As $t \to T$, the process drives the data toward a stationary distribution.
		\eqpause
		\item We design the transition matrices $\bQ_t$ to achieve this behavior.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al. Structured denoising diffusion models in discrete state-spaces, 2021.}

	\begin{itemize}
		\item The choice of $\bQ_t$ determines how information is erased and what the stationary distribution becomes.
		\eqpause
		\item $\bQ_t$ and $\bQ_{1:t}$ should be easy to compute for each $t$.
		\item \textbf{Uniform diffusion}
		\[
			\bQ_t = (1 - \beta_t) \bI + \beta_t \bU,
			\qquad
			\mathbf{U}_{ij} = \tfrac{1}{K}.
		\]
		Each token is replaced by a uniformly random symbol with probability $\beta_t$.
		The stationary distribution is uniform noise.
		\eqpause
		\item \textbf{Absorbing diffusion}
		\[
			\bQ_t = (1-\beta_t)\bI + \beta_t\, \mathbf{e}_m \mathbf{1}^\top.
		\]
		Tokens are gradually replaced by a special mask $m$;
		the stationary distribution is fully masked.
	\end{itemize}

\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\[
		q(\bx_t|\bx_0) = \Cat(\bQ_{1:t}\bx_0),
		\qquad
		\bQ_{1:t} = \bQ_t \bQ_{t-1} \cdots \bQ_1.
	\]
	\eqpause
	\begin{block}{Uniform Diffusion}
		\[
			\bQ_t = (1-\beta_t)\bI + \beta_t \mathbf{U},
			\qquad
			\mathbf{U}_{ij} = \tfrac{1}{K}.
		\]
		\[
			\bQ_{1:t} = \bar\alpha_t\bI + (1 - \bar\alpha_t) \mathbf{U},
			\quad
			\bar\alpha_t = \prod_{s=1}^t (1-\beta_s).
		\]
		\eqpause
		\begin{itemize}
			\item Each token retains its original value with prob.~$\bar\alpha_t$.
			\item It becomes uniformly random with prob.~$(1 - \bar\alpha_t)$.
			\item As $t \to T$, the process converges to the stationary uniform distribution.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}

	\begin{block}{Absorbing Diffusion}
		\[
			\bQ_t = (1-\beta_t)\bI + \beta_t\, \mathbf{e}_m \mathbf{1}^\top,
		\]
		\[
			\bQ_{1:t} = \bar\alpha_t\,\bI + (1-\bar\alpha_t)\,\mathbf{e}_m \mathbf{1}^\top,
			\qquad
			\bar\alpha_t = \prod_{s=1}^t (1-\beta_s).
		\]
		\eqpause
		\begin{itemize}
			\item Each token retains its original value with prob.~$\bar\alpha_t$.
			\item It becomes $\mathbf{e}_m$ with prob.~$(1 - \bar\alpha_t)$.
			\item As $t \to T$, all tokens converge to the mask state: $q(\bx_T) \approx \Cat(\mathbf{e}_m)$.
			\item This makes the process analogous to \textbf{masked language modeling}.
		\end{itemize}
	\end{block}
\end{frame}
%=======
\begin{frame}{Uniform vs. Absorbing Transition Matrix}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., Structured denoising diffusion models in discrete state-spaces, 2021.}
	\begin{table}[h!]
		\centering
		\small
		\begin{tabular}{lcc}
			\toprule
			\textbf{Aspect} & \textbf{Uniform Diffusion} & \textbf{Absorbing Diffusion} \\
			\midrule
			$\bQ_t$
				& $(1-\beta_t)\bI + \beta_t \mathbf{U}$ 
				& $(1-\beta_t)\bI + \beta_t \mathbf{e}_m\mathbf{1}^\top$ \\[4pt]
			$\bQ_{1:t}$
				& $\bar\alpha_t \bI + (1-\bar\alpha_t)\mathbf{U}$
				& $\bar\alpha_t \bI + (1-\bar\alpha_t)\mathbf{e}_m\mathbf{1}^\top$ \\[4pt]
			$\bQ_{1:\infty}$
				& $\mathbf{U}$
				& $\Cat(\mathbf{e}_m)$ \\[4pt]
			Interpretation
				& Random replacement
				& Gradual masking of tokens \\[4pt]
			Application
				& Image / symbol diffusion
				& Text diffusion $\approx$ Masked LM \\[4pt]
			\bottomrule
		\end{tabular}
	\end{table}
	\eqpause
	\begin{block}{Observation}
		Both schemes gradually destroy information, but differ in their stationary limit.
		Absorbing diffusion bridges diffusion and masked-language-model objectives.
	\end{block}
\end{frame}
%=======
\begin{frame}
	NOT READY
\end{frame}
%=======
\begin{frame}{Reverse Process and Model Parameterization}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., 2021.}

	\begin{block}{Goal}
		Learn a reverse model that reconstructs cleaner data from corrupted inputs:
		\[
			p_\theta(\bx_{t-1}|\bx_t)
			\approx q(\bx_{t-1}|\bx_t,\bx_0).
		\]
	\end{block}
	\eqpause
	\begin{itemize}
		\item The reverse chain defines the generative process:
		\[
			p_\theta(\bx_{0:T}) = p(\bx_T)
			\prod_{t=1}^{T} p_\theta(\bx_{t-1}|\bx_t).
		\]
		\eqpause
		\item We parameterize $p_\theta(\bx_{t-1}|\bx_t)$ as a factorized categorical distribution:
		\[
			p_\theta(x_{t-1,i}|x_t)
			= \Cat(x_{t-1,i};\,\pi_\theta(x_t,i,t)),
		\]
		where $\pi_\theta$ are model logits over $K$ symbols.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Variational Objective (Discrete ELBO)}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., 2021.}
	\begin{block}{Evidence Lower Bound}
		\[
		\log p_\theta(\bx_0)
		\ge
		\mathbb{E}_q\!\Big[
			\sum_{t=1}^T
			-D_{\mathrm{KL}}\big(
				q(\bx_{t-1}|\bx_t,\bx_0)
				\;\|\;
				p_\theta(\bx_{t-1}|\bx_t)
			)
		\Big].
		\]
	\end{block}
	\eqpause
	For categorical transitions, the KL becomes a cross-entropy term:
	\[
		\mathcal{L}_t
		= \mathbb{E}_{\bx_0,t}
		[-\log p_\theta(x_{t-1}\!=\!x_0\,|\,\bx_t,t)].
	\]
	\eqpause
	\begin{itemize}
		\item Equivalent to predicting the clean token $x_0$ from a partially noised $\bx_t$.
		\item In practice, the model learns to *denoise* corrupted inputs at multiple noise levels.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Relation to Masked Language Modeling (MLM)}
	\myfootnotewithlink{https://arxiv.org/abs/2107.03006}{Austin J. et al., 2021.}
	\begin{itemize}
		\item In absorbing diffusion, corrupted tokens are replaced by a mask $m$.
		\item The denoising task becomes identical to predicting masked tokens:
			\[
				\mathcal{L}
				= \mathbb{E}_{t\sim p(t)}
				\mathbb{E}_{\bx_t\sim q(\bx_t|\bx_0)}
				[-\log p_\theta(\bx_0|\bx_t,t)].
			\]
		\eqpause
		\item Therefore, discrete diffusion can be seen as a \textbf{mixture of MLM objectives}
			with varying masking rates.
		\eqpause
		\item This view directly connects diffusion LMs to BERT-style training,
			but provides a principled probabilistic framework.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Summary}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}
\end{document}